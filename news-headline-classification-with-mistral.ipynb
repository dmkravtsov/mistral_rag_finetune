{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4256ae8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-10T19:27:50.497335Z",
     "iopub.status.busy": "2024-11-10T19:27:50.496962Z",
     "iopub.status.idle": "2024-11-10T19:27:51.215948Z",
     "shell.execute_reply": "2024-11-10T19:27:51.214726Z"
    },
    "papermill": {
     "duration": 0.72542,
     "end_time": "2024-11-10T19:27:51.218519",
     "exception": false,
     "start_time": "2024-11-10T19:27:50.493099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/news-cats/cnn_news4cats.csv\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c2ab00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T19:27:51.224691Z",
     "iopub.status.busy": "2024-11-10T19:27:51.224253Z",
     "iopub.status.idle": "2024-11-10T19:29:55.231660Z",
     "shell.execute_reply": "2024-11-10T19:29:55.230507Z"
    },
    "papermill": {
     "duration": 124.01302,
     "end_time": "2024-11-10T19:29:55.234171",
     "exception": false,
     "start_time": "2024-11-10T19:27:51.221151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\r\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.44.1\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c244f9bcda824c399fc6d68a7fc350a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and initialized in 77.96 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Install bitsandbytes, a library for efficient GPU-based matrix operations.\n",
    "!pip install -U bitsandbytes\n",
    "\n",
    "# Install the latest versions of Transformers and Accelerate libraries for model handling and optimization.\n",
    "!pip install --upgrade transformers accelerate -q\n",
    "\n",
    "# Disable parallelism in tokenizers to prevent warnings or conflicts during model loading.\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Import necessary libraries for data handling, model operations, and evaluation.\n",
    "import pandas as pd  # Used for handling and processing tabular data (e.g., CSV files).\n",
    "from sklearn.preprocessing import LabelEncoder  # Transforms categorical labels into numerical format.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # Core libraries for working with Hugging Face models and configurations.\n",
    "import torch  # PyTorch framework for handling tensors and GPU computations.\n",
    "import time  # Used to log and calculate the time taken for operations.\n",
    "\n",
    "# Step 1: Load the dataset containing news headlines and their categories.\n",
    "data = pd.read_csv('/kaggle/input/news-cats/cnn_news4cats.csv')  # Load dataset from a specified path.\n",
    "\n",
    "# Convert textual categories (e.g., 'sports', 'politics') to numerical labels.\n",
    "label_encoder = LabelEncoder()  # Initialize a LabelEncoder instance.\n",
    "data['category_encoded'] = label_encoder.fit_transform(data['category'])  # Fit and transform the category column.\n",
    "\n",
    "# Extract headlines and corresponding encoded labels for classification tasks.\n",
    "texts = data['titles']  # Extract news headlines (input data).\n",
    "labels = data['category_encoded']  # Extract encoded labels (target data).\n",
    "\n",
    "# Log the time and initialize model/tokenizer loading.\n",
    "start_time = time.time()  # Start timer to measure loading time.\n",
    "\n",
    "# Define the path to the pre-trained model (Mistral 7B, a large language model for instruction tasks).\n",
    "model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n",
    "\n",
    "# Load the tokenizer that converts input text into tokens suitable for the model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  \n",
    "# `trust_remote_code=True` allows using custom code provided by the model's developers.\n",
    "\n",
    "# Configure BitsAndBytes for 4-bit quantization to optimize model memory usage.\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)  \n",
    "# 4-bit quantization reduces model size while maintaining performance.\n",
    "\n",
    "# Load the pre-trained language model with 4-bit quantization and auto device placement.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Path to the model.\n",
    "    quantization_config=bnb_config,  # Apply quantization settings.\n",
    "    trust_remote_code=True,  # Trust any custom model-specific code.\n",
    "    device_map=\"auto\",  # Automatically map model components to available devices (CPU/GPU).\n",
    "    local_files_only=True  # Use only local files, avoiding network downloads.\n",
    ")\n",
    "\n",
    "# Set the model to evaluation mode, which disables training-specific layers like dropout.\n",
    "model.eval()\n",
    "\n",
    "# Log the total time taken to load and initialize the model and tokenizer.\n",
    "print(f\"Model loaded and initialized in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Set the tokenizer's padding token to the model's end-of-sequence token.\n",
    "# This ensures consistent padding for inputs of varying lengths.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a96405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T19:29:55.247604Z",
     "iopub.status.busy": "2024-11-10T19:29:55.246996Z",
     "iopub.status.idle": "2024-11-10T19:41:19.048641Z",
     "shell.execute_reply": "2024-11-10T19:41:19.047597Z"
    },
    "papermill": {
     "duration": 683.816385,
     "end_time": "2024-11-10T19:41:19.056578",
     "exception": false,
     "start_time": "2024-11-10T19:29:55.240193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for the sample: 0.8744585647252424\n",
      "Headline: Fan gambling heard by players ‘every single round,’ says Jon Rahm\n",
      "True Category: sports, Predicted Category: sports\n",
      "\n",
      "Headline: Potential VP picks for Harris running mate\n",
      "True Category: politics, Predicted Category: politics\n",
      "\n",
      "Headline: 5 stories to start your day\n",
      "True Category: sports, Predicted Category: Unrecognized\n",
      "\n",
      "Headline: St. Louis rain continues, heat in the Pacific Northwest, and monsoon rain for Southwest\n",
      "True Category: weather, Predicted Category: weather\n",
      "\n",
      "Headline: Is it normal for cold-like symptoms to last for weeks? An expert explains\n",
      "True Category: health, Predicted Category: health\n",
      "\n",
      "Headline: Trump claims not to know who is behind Project 2025. A CNN review found at least 140 people who worked for him are involved\n",
      "True Category: politics, Predicted Category: politics\n",
      "\n",
      "Headline: Axelrod thinks Trump should be worried about Harris. Here's why\n",
      "True Category: politics, Predicted Category: politics\n",
      "\n",
      "Headline: Record-breaking Kenyan climber wants to advance the sport in his country\n",
      "True Category: sports, Predicted Category: sports\n",
      "\n",
      "Headline: Does your boss care about you? Here’s what most workers say\n",
      "True Category: health, Predicted Category: politics\n",
      "\n",
      "Headline: Joe Manchin and Rob Portman hold back support from their parties’ presidential nominees while calling for bipartisanship in Congress\n",
      "True Category: politics, Predicted Category: politics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch  # Core library for tensor computations and model operations.\n",
    "from sklearn.metrics import f1_score  # Used for calculating the F1 score, a classification evaluation metric.\n",
    "import random  # Provides tools to generate random numbers and random selections.\n",
    "\n",
    "# Step 3: Function to generate predictions using prompting.\n",
    "def classify_with_prompt(model, tokenizer, headlines, label_encoder):\n",
    "    \"\"\"\n",
    "    Function to classify a list of headlines using a language model and prompting.\n",
    "\n",
    "    Args:\n",
    "    - model: Pre-trained language model.\n",
    "    - tokenizer: Tokenizer to convert text into tokens for the model.\n",
    "    - headlines: List of news headlines to classify.\n",
    "    - label_encoder: Encoder to map categories to numeric labels.\n",
    "\n",
    "    Returns:\n",
    "    - predicted_labels: List of predicted label indices.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout and other training-specific layers).\n",
    "    predicted_labels = []  # Initialize an empty list to store predictions.\n",
    "\n",
    "    for headline in headlines:\n",
    "        # Construct a prompt instructing the model to classify the headline.\n",
    "        prompt = (\n",
    "            f\"Classify the following headline into one of these categories: \"\n",
    "            f\"{list(label_encoder.classes_)}.\\n\"\n",
    "            f\"Headline: '{headline}'\\nCategory:\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize the prompt and convert it to tensors suitable for model input.\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to('cuda')\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation to save memory and improve inference speed.\n",
    "            # Generate model output using the prompt.\n",
    "            outputs = model.generate(**inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        # Decode the generated output tokens back into a string.\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the predicted category from the generated text.\n",
    "        category = generated_text.split(\"Category:\")[-1].strip().lower()\n",
    "\n",
    "        # Post-process the predicted category to map it to a numeric label.\n",
    "        if category in label_encoder.classes_:\n",
    "            predicted_labels.append(label_encoder.transform([category])[0])\n",
    "        else:\n",
    "            predicted_labels.append(-1)  # Assign -1 for unrecognized categories.\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Step 4: Apply the model to a random sample of 20 examples from the dataset.\n",
    "sample_size = 1000  # Number of random samples to classify.\n",
    "sample_indices = random.sample(range(len(texts)), sample_size)  # Randomly select indices from the dataset.\n",
    "sample_texts = texts.iloc[sample_indices].tolist()  # Get the corresponding headlines.\n",
    "sample_labels = labels.iloc[sample_indices].tolist()  # Get the true labels for the selected headlines.\n",
    "\n",
    "# Classify the sampled headlines using the prompting method.\n",
    "predicted = classify_with_prompt(model, tokenizer, sample_texts, label_encoder)\n",
    "\n",
    "# Step 5: Calculate the F1 score for the sample.\n",
    "f1 = f1_score(sample_labels, predicted, average='weighted', zero_division=1)\n",
    "print(f\"F1 Score for the sample: {f1}\")  # Print the F1 score for the sample.\n",
    "\n",
    "# Display 10 random predictions for inspection.\n",
    "display_indices = random.sample(range(sample_size), 10)  # Randomly select 10 indices from the sample.\n",
    "for i in display_indices:\n",
    "    headline = sample_texts[i]  # Get the corresponding headline.\n",
    "    true_label = sample_labels[i]  # Get the true label.\n",
    "    pred_label = predicted[i]  # Get the predicted label.\n",
    "    \n",
    "    # Print the headline, true category, and predicted category.\n",
    "    print(f\"Headline: {headline}\")\n",
    "    print(f\"True Category: {label_encoder.inverse_transform([true_label])[0]}, \"\n",
    "          f\"Predicted Category: {label_encoder.inverse_transform([pred_label])[0] if pred_label != -1 else 'Unrecognized'}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6051679,
     "sourceId": 9860571,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 813.986656,
   "end_time": "2024-11-10T19:41:21.794185",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-10T19:27:47.807529",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07ded0e1377f474cb771c0a93a13cdf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "358e036b2faf451ca27f97bdf5296d29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3815e1b6505a47f4a01df07a8ad955c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "529d1cf76a284701a5f23810956b1cca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3815e1b6505a47f4a01df07a8ad955c0",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_07ded0e1377f474cb771c0a93a13cdf0",
       "value": 2.0
      }
     },
     "60b7f85a3541467b8f27e997a3fa5c8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_358e036b2faf451ca27f97bdf5296d29",
       "placeholder": "​",
       "style": "IPY_MODEL_8bdf5e679d754ebf9ec5644bc484002b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "8bdf5e679d754ebf9ec5644bc484002b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "955baed788b74af18f7944c0a6a957e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c244f9bcda824c399fc6d68a7fc350a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_60b7f85a3541467b8f27e997a3fa5c8f",
        "IPY_MODEL_529d1cf76a284701a5f23810956b1cca",
        "IPY_MODEL_d0c926bd2cb7459daee9e45e27029612"
       ],
       "layout": "IPY_MODEL_955baed788b74af18f7944c0a6a957e0"
      }
     },
     "d0c926bd2cb7459daee9e45e27029612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fdbf8a30b52a4893bda314dd05ebeff8",
       "placeholder": "​",
       "style": "IPY_MODEL_f7e7019fdfb84697b5927e0b906de0ec",
       "value": " 2/2 [01:03&lt;00:00, 29.72s/it]"
      }
     },
     "f7e7019fdfb84697b5927e0b906de0ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fdbf8a30b52a4893bda314dd05ebeff8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
