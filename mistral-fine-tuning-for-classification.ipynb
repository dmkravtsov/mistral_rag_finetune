{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aad8079",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-12T15:40:32.044249Z",
     "iopub.status.busy": "2024-11-12T15:40:32.043948Z",
     "iopub.status.idle": "2024-11-12T15:40:32.750347Z",
     "shell.execute_reply": "2024-11-12T15:40:32.749477Z"
    },
    "papermill": {
     "duration": 0.713592,
     "end_time": "2024-11-12T15:40:32.752585",
     "exception": false,
     "start_time": "2024-11-12T15:40:32.038993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n",
      "/kaggle/input/news-cats/cnn_news4cats.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc8a10",
   "metadata": {
    "papermill": {
     "duration": 0.003426,
     "end_time": "2024-11-12T15:40:32.760052",
     "exception": false,
     "start_time": "2024-11-12T15:40:32.756626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Summary\n",
    "This project focuses on fine-tuning a large language model for text classification tasks, leveraging techniques like quantization and low-rank adaptation. The goal is to optimize the model for memory efficiency and fast training without compromising performance.\n",
    "\n",
    "### Explanation of Components\n",
    "\n",
    "**Bits and Bytes Configuration (bnb_config)**  \n",
    "**Role:** \"Engine compression\"  \n",
    "**Purpose:** Compresses model weights to a 4-bit format for efficient memory usage and faster computation. This is crucial for working with large models on GPUs with limited memory.  \n",
    "**Interaction:** Prepares model weights for efficient processing, enabling compatibility with other fine-tuning components.\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)**  \n",
    "**Role:** \"Transmission\"  \n",
    "**Purpose:** Allows fine-tuning by training small, low-rank matrices instead of updating all model parameters. This reduces memory usage and speeds up training.  \n",
    "**Interaction:** LoRA adds adaptive weights to selected layers of the model, improving its ability to learn from new data.\n",
    "\n",
    "**TrainingArguments**  \n",
    "**Role:** \"Driving mode setup\"  \n",
    "**Purpose:** Specifies key training parameters like batch size, learning rate, and evaluation frequency.  \n",
    "**Interaction:** Manages the fine-tuning process through SFTTrainer, ensuring efficient training and validation.\n",
    "\n",
    "**SFTTrainer**  \n",
    "**Role:** \"Learning brain\"  \n",
    "**Purpose:** Central controller that orchestrates model training, evaluation, and saving. It integrates all configurations, including bnb_config, LoRA, and TrainingArguments.  \n",
    "**Interaction:** Coordinates between the model, dataset, and training strategy to ensure smooth execution.\n",
    "\n",
    "**Training Workflow**  \n",
    "**Overview:**  \n",
    "1. **bnb_config** compresses the model for efficient resource usage.\n",
    "2. **LoRA** adds lightweight adaptive layers to the model.\n",
    "3. **TrainingArguments** sets the training rules.\n",
    "4. **SFTTrainer** executes the training process, applying configurations and managing data flow.  \n",
    "\n",
    "Together, these components form a streamlined pipeline for efficient fine-tuning, analogous to optimizing a car for performance:\n",
    "- **bnb_config**: Lightens the vehicle.\n",
    "- **LoRA**: Enhances transmission efficiency.\n",
    "- **TrainingArguments**: Defines driving conditions.\n",
    "- **SFTTrainer**: Acts as the driver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f748116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:40:32.768244Z",
     "iopub.status.busy": "2024-11-12T15:40:32.767849Z",
     "iopub.status.idle": "2024-11-12T15:41:46.428085Z",
     "shell.execute_reply": "2024-11-12T15:41:46.427272Z"
    },
    "papermill": {
     "duration": 73.667007,
     "end_time": "2024-11-12T15:41:46.430485",
     "exception": false,
     "start_time": "2024-11-12T15:40:32.763478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\r\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.44.1\r\n",
      "Collecting peft\r\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: peft\r\n",
      "Successfully installed peft-0.13.2\r\n",
      "Collecting trl\r\n",
      "  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\r\n",
      "Requirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\r\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\r\n",
      "Collecting transformers>=4.46.0 (from trl)\r\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.25.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.20.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\r\n",
      "Downloading trl-0.12.0-py3-none-any.whl (310 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: transformers, trl\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "Successfully installed transformers-4.46.2 trl-0.12.0\r\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries for model fine-tuning\n",
    "!pip install -U bitsandbytes  # Install bitsandbytes for efficient model quantization\n",
    "!pip install peft  # Install PEFT (Parameter-Efficient Fine-Tuning) for LoRA\n",
    "!pip install trl  # Install TRL (Transformers Reinforcement Learning) for SFTTrainer\n",
    "\n",
    "# Importing required modules\n",
    "import os  # To manage environment variables and file paths\n",
    "import warnings  # To suppress unnecessary warnings\n",
    "import pandas as pd  # For handling tabular data\n",
    "import torch  # PyTorch for model operations and GPU computations\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset into train, validation, and test\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig  # Transformers for model and tokenizer\n",
    "from datasets import Dataset  # For dataset manipulation in HuggingFace format\n",
    "from peft import LoraConfig  # For LoRA configurations\n",
    "from trl import SFTTrainer  # For fine-tuning causal language models efficiently\n",
    "\n",
    "# Setting up system configurations\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure the code uses GPU 0 for computations\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer parallelism to avoid threading issues\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all warnings for cleaner outputs\n",
    "\n",
    "# Explanation of Key Components:\n",
    "# 1. **Bits and Bytes**: Optimizes large models using low-bit quantization, reducing memory usage and improving speed.\n",
    "# 2. **PEFT (LoRA)**: Parameter-efficient fine-tuning module to reduce training overhead and memory consumption.\n",
    "# 3. **TRL (SFTTrainer)**: Simplifies the fine-tuning process for large language models with flexible configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5c0e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:41:46.451436Z",
     "iopub.status.busy": "2024-11-12T15:41:46.451090Z",
     "iopub.status.idle": "2024-11-12T15:41:46.711666Z",
     "shell.execute_reply": "2024-11-12T15:41:46.710846Z"
    },
    "papermill": {
     "duration": 0.273862,
     "end_time": "2024-11-12T15:41:46.714211",
     "exception": false,
     "start_time": "2024-11-12T15:41:46.440349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
    "from datasets import Dataset  # HuggingFace Dataset for efficient data handling\n",
    "import pandas as pd  # For handling tabular data\n",
    "SEED = 2020  # Setting a fixed seed for reproducibility\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('/kaggle/input/news-cats/cnn_news4cats.csv')  # Load news dataset\n",
    "\n",
    "# Sampling the dataset to 1000 records for training efficiency\n",
    "# df = df.sample(1000, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Splitting the dataset into train, validation, and test subsets\n",
    "train_data, temp_data = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2,  # Reserve 20% for validation and test\n",
    "    stratify=df['category'],  # Stratify to maintain category distribution\n",
    "    random_state=SEED\n",
    ")\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, \n",
    "    test_size=0.5,  # Split the remaining 20% equally into validation and test\n",
    "    stratify=temp_data['category'], \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Function to generate training prompts with ground truth\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "        [INST]Classify the news headline: [{data_point['titles']}]. \n",
    "        Choose one of the following categories: health, politics, sports, weather.[/INST]\n",
    "        Correct category: {data_point['category']}\n",
    "    \"\"\".strip()\n",
    "\n",
    "# Function to generate testing prompts without ground truth\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "        [INST]Classify the news headline: [{data_point['titles']}]. \n",
    "        Choose one of the following categories: health, politics, sports, weather.[/INST]\n",
    "        Correct category: \n",
    "    \"\"\".strip()\n",
    "\n",
    "# Applying prompt generation to the datasets\n",
    "train_data['prompt'] = train_data.apply(generate_prompt, axis=1)  # Add prompts for training\n",
    "val_data['prompt'] = val_data.apply(generate_prompt, axis=1)  # Add prompts for validation\n",
    "test_data['prompt'] = test_data.apply(generate_test_prompt, axis=1)  # Add prompts for testing\n",
    "\n",
    "# Converting dataframes to HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data[['prompt', 'category']])  # Training dataset with prompts and categories\n",
    "val_dataset = Dataset.from_pandas(val_data[['prompt', 'category']])  # Validation dataset with prompts and categories\n",
    "test_dataset = Dataset.from_pandas(test_data[['prompt', 'category']])  # Test dataset with prompts and categories\n",
    "\n",
    "# Explanation of Key Components:\n",
    "# 1. **train_test_split**: Splits the dataset while maintaining category distribution.\n",
    "# 2. **generate_prompt & generate_test_prompt**: Create input prompts for training and testing.\n",
    "# 3. **Dataset.from_pandas**: Converts pandas DataFrame to HuggingFace Dataset, optimizing for ML workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f0c892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:41:46.735827Z",
     "iopub.status.busy": "2024-11-12T15:41:46.735496Z",
     "iopub.status.idle": "2024-11-12T15:43:08.302090Z",
     "shell.execute_reply": "2024-11-12T15:43:08.301042Z"
    },
    "papermill": {
     "duration": 81.579517,
     "end_time": "2024-11-12T15:43:08.304558",
     "exception": false,
     "start_time": "2024-11-12T15:41:46.725041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b82c9c8466e458b85f49273cbafe3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model path where the pretrained model is stored\n",
    "model_path = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n",
    "\n",
    "# Configuration for quantization using Bits and Bytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enables 4-bit quantization to reduce memory usage\n",
    "    bnb_4bit_use_double_quant=False,  # Disable double quantization for simplicity\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type as NF4 (Non-Finite quantization for better precision)\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Use half-precision floating point for computation to save memory\n",
    ")\n",
    "\n",
    "# Load the pretrained model for causal language modeling with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,  # Path to the model\n",
    "    device_map=\"auto\",  # Automatically maps model to available GPU/CPU\n",
    "    quantization_config=bnb_config  # Apply the quantization configuration\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,  # Path to the model tokenizer\n",
    "    padding_side=\"left\"  # Add padding on the left side of sequences\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as pad token for consistency\n",
    "\n",
    "# Key Components Explained:\n",
    "# 1. **bnb_config**:\n",
    "#    - Optimizes memory usage by reducing precision (4-bit quantization).\n",
    "#    - Improves inference speed and fits large models on limited GPU memory.\n",
    "#\n",
    "# 2. **AutoModelForCausalLM**:\n",
    "#    - Loads a causal language model for tasks such as text generation and classification.\n",
    "#    - Quantization settings ensure the model runs efficiently on hardware with limited memory.\n",
    "#\n",
    "# 3. **AutoTokenizer**:\n",
    "#    - Handles tokenization of input text to prepare it for the model.\n",
    "#    - `padding_side=\"left\"` ensures padding tokens are added to the left, which is useful for causal models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5696d3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:43:08.325719Z",
     "iopub.status.busy": "2024-11-12T15:43:08.325357Z",
     "iopub.status.idle": "2024-11-12T16:35:31.992336Z",
     "shell.execute_reply": "2024-11-12T16:35:31.991401Z"
    },
    "papermill": {
     "duration": 3143.679585,
     "end_time": "2024-11-12T16:35:31.994351",
     "exception": false,
     "start_time": "2024-11-12T15:43:08.314766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1ebebd1d7649acb0407b65f98bf717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea065bdabb55448287beb457cad20e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 51:58, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.548100</td>\n",
       "      <td>1.917166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.414200</td>\n",
       "      <td>1.297427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.107300</td>\n",
       "      <td>1.038929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.014200</td>\n",
       "      <td>1.011084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.017300</td>\n",
       "      <td>1.002686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.6062035846710205, metrics={'train_runtime': 3140.9579, 'train_samples_per_second': 2.038, 'train_steps_per_second': 0.032, 'total_flos': 1.7863253930016768e+16, 'train_loss': 1.6062035846710205, 'epoch': 0.8307372793354102})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Отключаем W&B\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Конфигурация LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank for LoRA (can tune between 4 to 32 for different memory vs. performance trade-offs)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA (could try 16, 64 for tuning)\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\"],  # Key attention modules\n",
    "    lora_dropout=0.05,  # Dropout rate for LoRA (can experiment with 0.1, 0.2)\n",
    "    bias=\"none\",  # 'none', 'all' or 'lora_only' - controls where biases are added\n",
    "    task_type=\"CAUSAL_LM\"  # Type of task, remains fixed for causal language models\n",
    ")\n",
    "\n",
    "# Обновленная конфигурация TrainingArguments\n",
    "# Конфигурация TrainingArguments с оптимизацией гиперпараметров\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,  # Enables mixed-precision training to save memory and speed up (can disable for full precision)\n",
    "    per_device_train_batch_size=8,  # Batch size per GPU (try 4, 16 depending on GPU memory)\n",
    "    per_device_eval_batch_size=8,  # Same as above for evaluation (try different sizes to match training)\n",
    "    gradient_accumulation_steps=8,  # Number of steps to accumulate gradients (try 4, 16 to adjust learning dynamics)\n",
    "    max_steps=100,  # Maximum training steps (try increasing for larger datasets, e.g., 100 or 200)\n",
    "    learning_rate=5e-5,  # Learning rate (test with 3e-5, 1e-4 to adjust learning stability)\n",
    "    weight_decay=0.01,  # Regularization (can try 0.05, 0.1 to control overfitting)\n",
    "    logging_steps=10,  # Frequency of logging (useful for tracking; try 20 or 50 for less verbosity)\n",
    "    save_steps=20,  # How often to save checkpoints (adjust based on experiment duration; e.g., 20)\n",
    "    evaluation_strategy=\"steps\",  # Evaluate after a set number of steps\n",
    "    eval_steps=20,  # Frequency of evaluation (adjust with save_steps; e.g., 20 or 50)\n",
    "    load_best_model_at_end=True,  # Automatically loads the best model based on validation\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to track (can change to 'accuracy' if applicable)\n",
    "    greater_is_better=False,  # Lower eval_loss is better\n",
    "    output_dir=\"./results\",  # Directory to save results\n",
    "    save_total_limit=1,  # Limit the number of saved checkpoints (e.g., 2 or 3 for more flexibility)\n",
    "    run_name=\"optimized_experiment\",  # Name for this experiment (helps with logging/debugging)\n",
    "    report_to=[]  # Disables reporting to external trackers like W&B\n",
    ")\n",
    "\n",
    "# SFTTrainer: Handles fine-tuning logic\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Preloaded model for fine-tuning\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=val_dataset,  # Validation dataset for evaluation during training\n",
    "    dataset_text_field=\"prompt\",  # Column to be used for text input in datasets\n",
    "    tokenizer=tokenizer,  # Tokenizer aligned with the model\n",
    "    args=training_args,  # Training hyperparameters\n",
    "    peft_config=lora_config  # Low-Rank Adaptation configuration\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314b1ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T16:35:32.016764Z",
     "iopub.status.busy": "2024-11-12T16:35:32.016436Z",
     "iopub.status.idle": "2024-11-12T16:43:15.532621Z",
     "shell.execute_reply": "2024-11-12T16:43:15.531533Z"
    },
    "papermill": {
     "duration": 463.540467,
     "end_time": "2024-11-12T16:43:15.545354",
     "exception": false,
     "start_time": "2024-11-12T16:35:32.004887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Weighted F1 Score: 1.00\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       1.00      1.00      1.00       212\n",
      "    politics       1.00      1.00      1.00       241\n",
      "      sports       1.00      1.00      1.00       250\n",
      "     weather       1.00      1.00      1.00       250\n",
      "\n",
      "    accuracy                           1.00       953\n",
      "   macro avg       1.00      1.00      1.00       953\n",
      "weighted avg       1.00      1.00      1.00       953\n",
      "\n",
      "Test Weighted F1 Score: 0.95\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.97      0.94      0.95       213\n",
      "    politics       0.95      0.97      0.96       246\n",
      "      sports       0.92      0.94      0.93       249\n",
      "     weather       0.97      0.94      0.96       250\n",
      "\n",
      "    accuracy                           0.95       958\n",
      "   macro avg       0.95      0.95      0.95       958\n",
      "weighted avg       0.95      0.95      0.95       958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Mapping of category names to numeric labels for evaluation purposes.\n",
    "category_map = {'health': 0, 'politics': 1, 'sports': 2, 'weather': 3}\n",
    "\n",
    "# Function to predict labels for a dataset in batches.\n",
    "def predict_labels_batch(dataset, batch_size=8):\n",
    "    \"\"\"\n",
    "    Predicts labels for the input dataset using batch processing.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Dataset containing prompts for classification.\n",
    "        batch_size (int): Number of prompts to process in one batch.\n",
    "        \n",
    "    Returns:\n",
    "        list: Predicted categories for each input prompt.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0, len(dataset['prompt']), batch_size):\n",
    "        # Extracts a batch of prompts from the dataset.\n",
    "        prompts = dataset['prompt'][i:i + batch_size]\n",
    "        \n",
    "        # Tokenizes the prompts and prepares them for the model.\n",
    "        inputs = tokenizer(list(prompts), return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        \n",
    "        # Generates predictions for the batch.\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        # Decodes the predictions and extracts the predicted category.\n",
    "        for output in outputs:\n",
    "            generated_text = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "            predicted_category = generated_text.split(\"Correct category:\")[-1].strip().strip(\".]\").strip()\n",
    "            predictions.append(predicted_category)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Function to evaluate the model's performance on the validation dataset.\n",
    "def evaluate_on_validation(val_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset and calculates classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        val_dataset (Dataset): Validation dataset containing prompts and true labels.\n",
    "    \"\"\"\n",
    "    # Predicts categories for the validation dataset.\n",
    "    predicted_val_categories = predict_labels_batch(val_dataset, batch_size=8)\n",
    "    \n",
    "    # Converts true labels from text to numeric format.\n",
    "    true_val_categories = [category_map[label] for label in val_dataset['category']]\n",
    "    \n",
    "    # Converts predicted categories to numeric format using the category map.\n",
    "    predicted_val_labels = [category_map.get(pred, -1) for pred in predicted_val_categories]\n",
    "\n",
    "    # Filters out invalid predictions (those not found in the category map).\n",
    "    valid_indices = np.array(predicted_val_labels) != -1\n",
    "    filtered_true_val = np.array(true_val_categories)[valid_indices]\n",
    "    filtered_pred_val = np.array(predicted_val_labels)[valid_indices]\n",
    "\n",
    "    # Calculates the weighted F1 Score for the validation dataset.\n",
    "    f1_val = f1_score(filtered_true_val, filtered_pred_val, average='weighted')\n",
    "    print(f\"Validation Weighted F1 Score: {f1_val:.2f}\")\n",
    "\n",
    "    # Prints a detailed classification report for the validation dataset.\n",
    "    print(\"\\nValidation Classification Report:\")\n",
    "    print(classification_report(filtered_true_val, filtered_pred_val, target_names=category_map.keys()))\n",
    "\n",
    "# Function to evaluate the model's performance on the test dataset.\n",
    "def evaluate_on_test(test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and calculates classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        test_dataset (Dataset): Test dataset containing prompts and true labels.\n",
    "    \"\"\"\n",
    "    # Predicts categories for the test dataset.\n",
    "    predicted_test_categories = predict_labels_batch(test_dataset, batch_size=8)\n",
    "    \n",
    "    # Converts true labels from text to numeric format.\n",
    "    true_test_categories = [category_map[label] for label in test_dataset['category']]\n",
    "    \n",
    "    # Converts predicted categories to numeric format using the category map.\n",
    "    predicted_test_labels = [category_map.get(pred, -1) for pred in predicted_test_categories]\n",
    "\n",
    "    # Filters out invalid predictions (those not found in the category map).\n",
    "    valid_indices = np.array(predicted_test_labels) != -1\n",
    "    filtered_true_test = np.array(true_test_categories)[valid_indices]\n",
    "    filtered_pred_test = np.array(predicted_test_labels)[valid_indices]\n",
    "\n",
    "    # Calculates the weighted F1 Score for the test dataset.\n",
    "    f1_test = f1_score(filtered_true_test, filtered_pred_test, average='weighted')\n",
    "    print(f\"Test Weighted F1 Score: {f1_test:.2f}\")\n",
    "\n",
    "    # Prints a detailed classification report for the test dataset.\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(filtered_true_test, filtered_pred_test, target_names=category_map.keys()))\n",
    "\n",
    "# Runs evaluation on both validation and test datasets.\n",
    "evaluate_on_validation(val_dataset)\n",
    "evaluate_on_test(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daebb43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T16:43:15.567882Z",
     "iopub.status.busy": "2024-11-12T16:43:15.567256Z",
     "iopub.status.idle": "2024-11-12T16:43:17.533534Z",
     "shell.execute_reply": "2024-11-12T16:43:17.532604Z"
    },
    "papermill": {
     "duration": 1.979744,
     "end_time": "2024-11-12T16:43:17.535517",
     "exception": false,
     "start_time": "2024-11-12T16:43:15.555773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Real Category</th>\n",
       "      <th>Predicted Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Significant severe weather is possible across ...</td>\n",
       "      <td>weather</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Oz supported health insurance mandates and...</td>\n",
       "      <td>health</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emergency doctor: We need help before it’s too...</td>\n",
       "      <td>health</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For the first time in 25 years, August did not...</td>\n",
       "      <td>weather</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The most outrageous golf fashion of 2023</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why I’m going to vaccinate my kids against Cov...</td>\n",
       "      <td>health</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Charlamagne tha God: America has zero protecti...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apple, Amazon and Google post earnings that di...</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can the U.S. and China set aside tensions to t...</td>\n",
       "      <td>weather</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The week that life in Dubai ground to a halt</td>\n",
       "      <td>weather</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News Real Category  \\\n",
       "0  Significant severe weather is possible across ...       weather   \n",
       "1  Dr. Oz supported health insurance mandates and...        health   \n",
       "2  Emergency doctor: We need help before it’s too...        health   \n",
       "3  For the first time in 25 years, August did not...       weather   \n",
       "4           The most outrageous golf fashion of 2023        sports   \n",
       "5  Why I’m going to vaccinate my kids against Cov...        health   \n",
       "6  Charlamagne tha God: America has zero protecti...      politics   \n",
       "7  Apple, Amazon and Google post earnings that di...        sports   \n",
       "8  Can the U.S. and China set aside tensions to t...       weather   \n",
       "9       The week that life in Dubai ground to a halt       weather   \n",
       "\n",
       "  Predicted Category  \n",
       "0            weather  \n",
       "1             health  \n",
       "2             health  \n",
       "3            weather  \n",
       "4             sports  \n",
       "5             health  \n",
       "6           politics  \n",
       "7             sports  \n",
       "8            weather  \n",
       "9            weather  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Function to predict labels for a subset of the validation dataset\n",
    "def generate_comparison_table(val_dataset, batch_size=10):\n",
    "    \"\"\"\n",
    "    Generates a comparison table for validation data.\n",
    "    \n",
    "    Args:\n",
    "        val_dataset (Dataset): Validation dataset.\n",
    "        batch_size (int): Number of samples to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Table with news text, real category, and predicted category.\n",
    "    \"\"\"\n",
    "    # Randomly select 10 samples from the validation dataset\n",
    "    random_indices = random.sample(range(len(val_dataset)), batch_size)\n",
    "    sample_data = val_dataset.select(random_indices)\n",
    "    \n",
    "    # Predict categories for the selected samples\n",
    "    predicted_categories = predict_labels_batch(sample_data, batch_size=batch_size)\n",
    "    \n",
    "    # Extract real categories\n",
    "    real_categories = sample_data['category']\n",
    "    \n",
    "    # Create a DataFrame to compare real and predicted categories\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'News': [prompt.split(\": [\")[1].split(\"].\")[0].strip() for prompt in sample_data['prompt']],\n",
    "        'Real Category': real_categories,\n",
    "        'Predicted Category': predicted_categories\n",
    "    })\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Generate the comparison table\n",
    "comparison_table = generate_comparison_table(val_dataset, batch_size=10)\n",
    "\n",
    "# Display the table\n",
    "comparison_table\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6051679,
     "sourceId": 9860571,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3771.126673,
   "end_time": "2024-11-12T16:43:20.547830",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-12T15:40:29.421157",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1eeae04ceee1461aae85ab7f3894fd64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "227b04e520c4468685b5f30ce7c54b94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_70a6eb2368174c25be0985c8bda5891a",
       "placeholder": "​",
       "style": "IPY_MODEL_275077e4ed9043a0a1782ae9ad173d3f",
       "value": " 7699/7699 [00:01&lt;00:00, 6221.68 examples/s]"
      }
     },
     "229caaaffb314459a090ba3a4294c5ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bf46c2ac235f420f8fc29885a0a439a9",
       "placeholder": "​",
       "style": "IPY_MODEL_50cc34e653654f7497079843d3041021",
       "value": "Map: 100%"
      }
     },
     "275077e4ed9043a0a1782ae9ad173d3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "36569076de3b43f2844ccd752ba358a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c7418a1ea0044c4847b5f5e79e626ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "422a4908ccd54eb6925d7151df5a139e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f88169ab15de45bdaead61b2171e41ff",
       "max": 7699.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f0e23b8dccd047c1a54eb879cb7ff316",
       "value": 7699.0
      }
     },
     "4599487145d340eea9ce47356bcc7e4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f670f736f8c64577a664ed626f0aeea6",
       "max": 962.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e842ac447a294acaa761b846637b9918",
       "value": 962.0
      }
     },
     "50cc34e653654f7497079843d3041021": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "70a6eb2368174c25be0985c8bda5891a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77daaa9626634253a850e069cd739bf9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8b6f62b815f344078aa856734c941967": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8da20602fc464106b770c4abd0cfaf8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9bd1a412c8c4bafa68a9a466604a1f5",
       "placeholder": "​",
       "style": "IPY_MODEL_1eeae04ceee1461aae85ab7f3894fd64",
       "value": " 962/962 [00:00&lt;00:00, 5905.02 examples/s]"
      }
     },
     "9b82c9c8466e458b85f49273cbafe3d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fa8a6b2cb9624c208ee294a9c0cb5281",
        "IPY_MODEL_c379be5352eb491a96d4f441ab755141",
        "IPY_MODEL_a4afe4373d4f4ee8a25077a30fb34b67"
       ],
       "layout": "IPY_MODEL_f006e1c936174b09affa92c186c294d7"
      }
     },
     "a4afe4373d4f4ee8a25077a30fb34b67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3c7418a1ea0044c4847b5f5e79e626ae",
       "placeholder": "​",
       "style": "IPY_MODEL_e12f19d5fac949af8412bdccfc16a24a",
       "value": " 2/2 [01:20&lt;00:00, 37.67s/it]"
      }
     },
     "b1b306979b4947188c113f004705b72d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8e6d192175543e4b60c2b86815addad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf46c2ac235f420f8fc29885a0a439a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf4a6cb7dcdb400b9aeebe54c5d9c765": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c00ed51e67a147c0ac4ba292449327c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c379be5352eb491a96d4f441ab755141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c00ed51e67a147c0ac4ba292449327c6",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_77daaa9626634253a850e069cd739bf9",
       "value": 2.0
      }
     },
     "d2d2dd463b574185931a181f54554aab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d9bd1a412c8c4bafa68a9a466604a1f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e12f19d5fac949af8412bdccfc16a24a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e842ac447a294acaa761b846637b9918": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ea065bdabb55448287beb457cad20e4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ebba14f411f0407ebe40aac95e7379b3",
        "IPY_MODEL_4599487145d340eea9ce47356bcc7e4d",
        "IPY_MODEL_8da20602fc464106b770c4abd0cfaf8b"
       ],
       "layout": "IPY_MODEL_36569076de3b43f2844ccd752ba358a4"
      }
     },
     "ea1ebebd1d7649acb0407b65f98bf717": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_229caaaffb314459a090ba3a4294c5ac",
        "IPY_MODEL_422a4908ccd54eb6925d7151df5a139e",
        "IPY_MODEL_227b04e520c4468685b5f30ce7c54b94"
       ],
       "layout": "IPY_MODEL_8b6f62b815f344078aa856734c941967"
      }
     },
     "ebba14f411f0407ebe40aac95e7379b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8e6d192175543e4b60c2b86815addad",
       "placeholder": "​",
       "style": "IPY_MODEL_d2d2dd463b574185931a181f54554aab",
       "value": "Map: 100%"
      }
     },
     "f006e1c936174b09affa92c186c294d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0e23b8dccd047c1a54eb879cb7ff316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f670f736f8c64577a664ed626f0aeea6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f88169ab15de45bdaead61b2171e41ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa8a6b2cb9624c208ee294a9c0cb5281": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bf4a6cb7dcdb400b9aeebe54c5d9c765",
       "placeholder": "​",
       "style": "IPY_MODEL_b1b306979b4947188c113f004705b72d",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
